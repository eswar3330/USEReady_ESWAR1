{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7Q5MJvK11d8FgoF2PD7r3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eswar3330/USEReady_ESWAR1/blob/main/USEReady_AIML_Assignment_1_Eswar_Reddy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m3I94k8mm1D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dec6e490-8d54-43da-e388-f003434bf19d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Setting up environment and starting text extraction ---\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.2.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "\n",
            "Libraries installed successfully.\n",
            "\n",
            "Defining text extraction functions...\n",
            "Text extraction functions defined.\n",
            "\n",
            "Batch processing documents and extracting text...\n",
            "train.csv and test.csv loaded successfully.\n",
            "\n",
            "Processing documents in 'data/train'...\n",
            "Warning: No matching document found for '24158401-Rental-Agreement' in 'data/train' with any supported extension. Skipping.\n",
            "Skipped 1 training documents due to missing files or extraction errors.\n",
            "Successfully extracted text from 9 training documents.\n",
            "\n",
            "file names of extracted training text:\n",
            "                                           File Name  \\\n",
            "0  6683127-House-Rental-Contract-GERALDINE-GALINA...   \n",
            "1  6683129-House-Rental-Contract-Geraldine-Galina...   \n",
            "2                        18325926-Rental-Agreement-1   \n",
            "4                          36199312-Rental-Agreement   \n",
            "5  44737744-Maddireddy-Bhargava-Reddy-Rental-Agre...   \n",
            "6                          47854715-RENTAL-AGREEMENT   \n",
            "7                          50070534-RENTAL-AGREEMENT   \n",
            "8                          54770958-Rental-Agreement   \n",
            "9                          54945838-Rental-Agreement   \n",
            "\n",
            "                                      extracted_text  \n",
            "0  House Rental Contract\\nKNOWN ALL MEN BY THESE ...  \n",
            "1  \\n\\n\\n\\n\\nHouse Rental Contract\\nKNOWN ALL MEN...  \n",
            "2  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRENTAL AGREEMENT\\nThis dee...  \n",
            "4  RENEWAL OF RENTAL AGREEMENT\\n\\nThis AGREEMENT ...  \n",
            "5  RENTfft\\tENT\\nThis Rental Agreement is made an...  \n",
            "6  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRENTAL AGR...  \n",
            "7  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
            "8  RENTAL AGREEMENT\\n\\nThis RENTAL AGREEMENT is m...  \n",
            "9  RENTAL AGREEMENT\\n\\nThis Rental Agreement made...  \n",
            "\n",
            "Processing documents in 'data/test'...\n",
            "Successfully extracted text from 4 test documents.\n",
            "\n",
            " file names of extracted test text:\n",
            "                             File Name  \\\n",
            "0            24158401-Rental-Agreement   \n",
            "1            95980236-Rental-Agreement   \n",
            "2  156155545-Rental-Agreement-Kns-Home   \n",
            "3           228094620-Rental-Agreement   \n",
            "\n",
            "                                      extracted_text  \n",
            "0  This rental agreement is made and executed on ...  \n",
            "1  RENTAL AGREEMENT\\n\\nThis Rental Agreement is m...  \n",
            "2  RENTAL AGREEMENT\\nThis deed of rental agreemen...  \n",
            "3  RENTAL AGREEMENT\\nThis Rental Agreement is mad...  \n",
            "\n",
            "--- Document Text Extraction Complete ---\n",
            "Stored 'train_extracted_df' (DataFrame)\n",
            "Stored 'test_extracted_df' (DataFrame)\n",
            "Stored 'train_df_gt' (DataFrame)\n",
            "Stored 'test_df_gt' (DataFrame)\n",
            "\n",
            "Extracted text DataFrames and ground truth CSVs stored for next phase.\n"
          ]
        }
      ],
      "source": [
        "# --- Phase 0 & 1: Project Setup & Document Text Extraction ---\n",
        "\n",
        "print(\"--- Setting up environment and starting text extraction ---\")\n",
        "\n",
        "# Install necessary libraries\n",
        "!sudo apt install -y tesseract-ocr\n",
        "!pip install pytesseract\n",
        "!pip install python-docx\n",
        "!pip install pandas numpy\n",
        "\n",
        "print(\"\\nLibraries installed successfully.\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "import docx\n",
        "\n",
        "# Define possible file extensions to try\n",
        "POSSIBLE_EXTENSIONS = ['.docx', '.png', '.jpg', '.jpeg']\n",
        "\n",
        "print(\"\\nDefining text extraction functions...\")\n",
        "\n",
        "def extract_text_from_docx(docx_path):\n",
        "    \"\"\"Extracts text from a .docx file.\"\"\"\n",
        "    try:\n",
        "        doc = docx.Document(docx_path)\n",
        "        full_text = []\n",
        "        for para in doc.paragraphs:\n",
        "            full_text.append(para.text)\n",
        "        return \"\\n\".join(full_text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from DOCX {os.path.basename(docx_path)}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_text_from_image(image_path):\n",
        "    \"\"\"Extracts text from an image using Tesseract OCR.\"\"\"\n",
        "    try:\n",
        "        img = Image.open(image_path)\n",
        "        text = pytesseract.image_to_string(img)\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from IMAGE {os.path.basename(image_path)}: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"Text extraction functions defined.\")\n",
        "\n",
        "\n",
        "# Batch Process Documents and Extract Text\n",
        "print(\"\\nBatch processing documents and extracting text...\")\n",
        "\n",
        "data_dir = 'data'\n",
        "train_docs_dir = os.path.join(data_dir, 'train')\n",
        "test_docs_dir = os.path.join(data_dir, 'test')\n",
        "\n",
        "# Load ground truth CSVs to get file names\n",
        "try:\n",
        "    train_df_gt = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
        "    test_df_gt = pd.read_csv(os.path.join(data_dir, 'test.csv'))\n",
        "    print(\"train.csv and test.csv loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: train.csv or test.csv not found in '{data_dir}'. Please check file organization.\")\n",
        "    exit()\n",
        "\n",
        "# Function to find the correct file path and extract text\n",
        "def find_file_and_extract_text(base_file_name, directory):\n",
        "    for ext in POSSIBLE_EXTENSIONS:\n",
        "        full_file_name = base_file_name + ext\n",
        "        file_path = os.path.join(directory, full_file_name)\n",
        "        if os.path.exists(file_path):\n",
        "            if ext == '.docx':\n",
        "                return extract_text_from_docx(file_path)\n",
        "            elif ext in ['.png', '.jpg', '.jpeg']:\n",
        "                return extract_text_from_image(file_path)\n",
        "    print(f\"Warning: No matching document found for '{base_file_name}' in '{directory}' with any supported extension. Skipping.\")\n",
        "    return None\n",
        "\n",
        "# Process training documents\n",
        "extracted_train_data = []\n",
        "print(f\"\\nProcessing documents in '{train_docs_dir}'...\")\n",
        "for index, row in train_df_gt.iterrows():\n",
        "    base_file_name = row['File Name']\n",
        "    extracted_text = find_file_and_extract_text(base_file_name, train_docs_dir)\n",
        "    extracted_train_data.append({'File Name': base_file_name, 'extracted_text': extracted_text})\n",
        "\n",
        "train_extracted_df = pd.DataFrame(extracted_train_data)\n",
        "# Filter out rows where text extraction failed (extracted_text is None)\n",
        "original_train_count = len(train_extracted_df)\n",
        "train_extracted_df = train_extracted_df.dropna(subset=['extracted_text'])\n",
        "skipped_train_count = original_train_count - len(train_extracted_df)\n",
        "if skipped_train_count > 0:\n",
        "    print(f\"Skipped {skipped_train_count} training documents due to missing files or extraction errors.\")\n",
        "\n",
        "print(f\"Successfully extracted text from {len(train_extracted_df)} training documents.\")\n",
        "print(\"\\nfile names of extracted training text:\")\n",
        "print(train_extracted_df)\n",
        "\n",
        "\n",
        "# Process test documents\n",
        "extracted_test_data = []\n",
        "print(f\"\\nProcessing documents in '{test_docs_dir}'...\")\n",
        "for index, row in test_df_gt.iterrows():\n",
        "    base_file_name = row['File Name']\n",
        "    extracted_text = find_file_and_extract_text(base_file_name, test_docs_dir)\n",
        "    extracted_test_data.append({'File Name': base_file_name, 'extracted_text': extracted_text})\n",
        "\n",
        "test_extracted_df = pd.DataFrame(extracted_test_data)\n",
        "# Filter out rows where text extraction failed (extracted_text is None)\n",
        "original_test_count = len(test_extracted_df)\n",
        "test_extracted_df = test_extracted_df.dropna(subset=['extracted_text'])\n",
        "skipped_test_count = original_test_count - len(test_extracted_df)\n",
        "if skipped_test_count > 0:\n",
        "    print(f\"Skipped {skipped_test_count} test documents due to missing files or extraction errors.\")\n",
        "\n",
        "print(f\"Successfully extracted text from {len(test_extracted_df)} test documents.\")\n",
        "print(\"\\n file names of extracted test text:\")\n",
        "print(test_extracted_df)\n",
        "\n",
        "print(\"\\n--- Document Text Extraction Complete ---\")\n",
        "\n",
        "# Store the extracted DataFrames for future phases\n",
        "%store train_extracted_df\n",
        "%store test_extracted_df\n",
        "%store train_df_gt\n",
        "%store test_df_gt\n",
        "print(\"\\nExtracted text DataFrames and ground truth CSVs stored for next phase.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Phase 2: Data Preprocessing & Ground Truth Alignment ---\n",
        "\n",
        "print(\"\\n--- Starting Data Preprocessing & Ground Truth Alignment ---\")\n",
        "\n",
        "# Recall stored DataFrames from previous phase\n",
        "%store -r train_extracted_df\n",
        "%store -r test_extracted_df\n",
        "%store -r train_df_gt\n",
        "%store -r test_df_gt\n",
        "\n",
        "print(\"Recalled stored DataFrames.\")\n",
        "\n",
        "# Merge Extracted Text with Ground Truth Metadata\n",
        "print(\"\\nMerging extracted text with ground truth metadata...\")\n",
        "\n",
        "# Merge training data\n",
        "# Using 'File Name' as the key for merging\n",
        "train_data = pd.merge(train_extracted_df, train_df_gt, on='File Name', how='inner')\n",
        "print(f\"Merged training data shape: {train_data.shape}\")\n",
        "print(\"\\nMerged Training Data Head:\")\n",
        "print(train_data.head())\n",
        "print(\"\\nMerged Training Data Info (check for missing values):\")\n",
        "train_data.info()\n",
        "\n",
        "# Merge testing data\n",
        "test_data = pd.merge(test_extracted_df, test_df_gt, on='File Name', how='inner')\n",
        "print(f\"\\nMerged testing data shape: {test_data.shape}\")\n",
        "print(\"\\nMerged Testing Data Head:\")\n",
        "print(test_data.head())\n",
        "print(\"\\nMerged Testing Data Info (check for missing values):\")\n",
        "test_data.info()\n",
        "\n",
        "\n",
        "# Define Target Fields and Create Standardized Field List\n",
        "print(\"\\nDefining target fields and standardizing list...\")\n",
        "\n",
        "# These are the fields we need to extract, as per the problem statement\n",
        "TARGET_FIELDS = [\n",
        "    'Aggrement Value',\n",
        "    'Aggrement Start Date',\n",
        "    'Aggrement End Date',\n",
        "    'Renewal Notice (Days)',\n",
        "    'Party One',\n",
        "    'Party Two'\n",
        "]\n",
        "\n",
        "print(f\"\\nTarget fields to extract: {TARGET_FIELDS}\")\n",
        "\n",
        "# Verify all target columns are present in the merged DataFrames\n",
        "for field in TARGET_FIELDS:\n",
        "    if field not in train_data.columns:\n",
        "        print(f\"Error: Target field '{field}' not found in train_data DataFrame. Check CSV columns.\")\n",
        "        exit()\n",
        "    if field not in test_data.columns:\n",
        "        print(f\"Error: Target field '{field}' not found in test_data DataFrame. Check CSV columns.\")\n",
        "        exit()\n",
        "print(\"All target fields confirmed present in merged DataFrames.\")\n",
        "\n",
        "\n",
        "# Handle Missing Ground Truth Values in Training Data\n",
        "print(\"\\nHandling missing ground truth values in training data...\")\n",
        "# For IE, if a ground truth value is missing in the CSV, it means that field is not present\n",
        "# in that particular document for training purposes. We will leave them as NaN.\n",
        "# During evaluation, we'll need to handle these correctly (e.g., if GT is NaN, it's not a matchable field).\n",
        "\n",
        "print(\"Missing ground truth values in train_data before further processing:\")\n",
        "print(train_data[TARGET_FIELDS].isnull().sum())\n",
        "print(\"\\nMissing ground truth values in test_data before further processing:\")\n",
        "print(test_data[TARGET_FIELDS].isnull().sum())\n",
        "\n",
        "# Insight: If 'Renewal Notice (Days)' has NaN, it might mean it's not applicable for some contracts.\n",
        "# We need to decide how to handle this during training/evaluation.\n",
        "# For exact match, an NaN in GT means we can't get a 'True' match for it.\n",
        "\n",
        "print(\"\\n--- Data Preprocessing & Ground Truth Alignment Complete ---\")\n",
        "\n",
        "# Store the merged DataFrames for the next phase\n",
        "%store train_data\n",
        "%store test_data\n",
        "%store TARGET_FIELDS\n",
        "print(\"Merged data and target fields stored for next phase.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rDlhfqexut9",
        "outputId": "5de9d6c6-4096-4815-db12-45f7ccb883ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Data Preprocessing & Ground Truth Alignment ---\n",
            "Recalled stored DataFrames.\n",
            "\n",
            "Merging extracted text with ground truth metadata...\n",
            "Merged training data shape: (9, 8)\n",
            "\n",
            "Merged Training Data Head:\n",
            "                                           File Name  \\\n",
            "0  6683127-House-Rental-Contract-GERALDINE-GALINA...   \n",
            "1  6683129-House-Rental-Contract-Geraldine-Galina...   \n",
            "2                        18325926-Rental-Agreement-1   \n",
            "3                          36199312-Rental-Agreement   \n",
            "4  44737744-Maddireddy-Bhargava-Reddy-Rental-Agre...   \n",
            "\n",
            "                                      extracted_text  Aggrement Value  \\\n",
            "0  House Rental Contract\\nKNOWN ALL MEN BY THESE ...             6500   \n",
            "1  \\n\\n\\n\\n\\nHouse Rental Contract\\nKNOWN ALL MEN...             6500   \n",
            "2  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRENTAL AGREEMENT\\nThis dee...             4000   \n",
            "3  RENEWAL OF RENTAL AGREEMENT\\n\\nThis AGREEMENT ...             3800   \n",
            "4  RENTfft\\tENT\\nThis Rental Agreement is made an...             3000   \n",
            "\n",
            "  Aggrement Start Date Aggrement End Date  Renewal Notice (Days)  \\\n",
            "0           20.05.2007         20.05.2008                   15.0   \n",
            "1           20.05.2007         20.05.2008                   15.0   \n",
            "2           05.12.2008         31.11.2009                   90.0   \n",
            "3           01.05.2010         31.04.2011                   30.0   \n",
            "4           20.09.2010         19.07.2011                    NaN   \n",
            "\n",
            "                                           Party One  \\\n",
            "0  Antonio Levy S. Ingles, Jr. and/or Mary Rose C...   \n",
            "1  Antonio Levy S. Ingles, Jr. and/or Mary Rose C...   \n",
            "2                                       MR.K.Kuttan    \n",
            "3                                          Balaji.R    \n",
            "4                              M.V.V. VIJAYA SHANKAR   \n",
            "\n",
            "                   Party Two  \n",
            "0      GERALDINE Q. GALINATO  \n",
            "1      GERALDINE Q. GALINATO  \n",
            "2   P.M. Narayana Namboodri   \n",
            "3                 Kartheek R  \n",
            "4  MADDIREDDY BHARGAVA REDDY  \n",
            "\n",
            "Merged Training Data Info (check for missing values):\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 9 entries, 0 to 8\n",
            "Data columns (total 8 columns):\n",
            " #   Column                 Non-Null Count  Dtype  \n",
            "---  ------                 --------------  -----  \n",
            " 0   File Name              9 non-null      object \n",
            " 1   extracted_text         9 non-null      object \n",
            " 2   Aggrement Value        9 non-null      int64  \n",
            " 3   Aggrement Start Date   9 non-null      object \n",
            " 4   Aggrement End Date     9 non-null      object \n",
            " 5   Renewal Notice (Days)  8 non-null      float64\n",
            " 6   Party One              9 non-null      object \n",
            " 7   Party Two              9 non-null      object \n",
            "dtypes: float64(1), int64(1), object(6)\n",
            "memory usage: 708.0+ bytes\n",
            "\n",
            "Merged testing data shape: (4, 8)\n",
            "\n",
            "Merged Testing Data Head:\n",
            "                             File Name  \\\n",
            "0            24158401-Rental-Agreement   \n",
            "1            95980236-Rental-Agreement   \n",
            "2  156155545-Rental-Agreement-Kns-Home   \n",
            "3           228094620-Rental-Agreement   \n",
            "\n",
            "                                      extracted_text  Aggrement Value  \\\n",
            "0  This rental agreement is made and executed on ...            12000   \n",
            "1  RENTAL AGREEMENT\\n\\nThis Rental Agreement is m...             9000   \n",
            "2  RENTAL AGREEMENT\\nThis deed of rental agreemen...            12000   \n",
            "3  RENTAL AGREEMENT\\nThis Rental Agreement is mad...            15000   \n",
            "\n",
            "  Aggrement Start Date Aggrement End Date  Renewal Notice (Days)  \\\n",
            "0           01.04.2008         31.03.2009                     60   \n",
            "1           01.04.2010         31.03.2011                     30   \n",
            "2           15.12.2012         14.11.2013                     30   \n",
            "3           07.07.2013         06.06.2014                     30   \n",
            "\n",
            "          Party One                                 Party Two  \n",
            "0        Hanumaiah                           Vishal Bhardwaj   \n",
            "1      S.Sakunthala                             V.V.Ravi Kian  \n",
            "2      V.K.NATARAJ    VYSHNAVI DAIRY SPECIALITIES Private Ltd  \n",
            "3   KAPIL MEHROTRA                                .B.Kishore   \n",
            "\n",
            "Merged Testing Data Info (check for missing values):\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4 entries, 0 to 3\n",
            "Data columns (total 8 columns):\n",
            " #   Column                 Non-Null Count  Dtype \n",
            "---  ------                 --------------  ----- \n",
            " 0   File Name              4 non-null      object\n",
            " 1   extracted_text         4 non-null      object\n",
            " 2   Aggrement Value        4 non-null      int64 \n",
            " 3   Aggrement Start Date   4 non-null      object\n",
            " 4   Aggrement End Date     4 non-null      object\n",
            " 5   Renewal Notice (Days)  4 non-null      int64 \n",
            " 6   Party One              4 non-null      object\n",
            " 7   Party Two              4 non-null      object\n",
            "dtypes: int64(2), object(6)\n",
            "memory usage: 388.0+ bytes\n",
            "\n",
            "Defining target fields and standardizing list...\n",
            "\n",
            "Target fields to extract: ['Aggrement Value', 'Aggrement Start Date', 'Aggrement End Date', 'Renewal Notice (Days)', 'Party One', 'Party Two']\n",
            "All target fields confirmed present in merged DataFrames.\n",
            "\n",
            "Handling missing ground truth values in training data...\n",
            "Missing ground truth values in train_data before further processing:\n",
            "Aggrement Value          0\n",
            "Aggrement Start Date     0\n",
            "Aggrement End Date       0\n",
            "Renewal Notice (Days)    1\n",
            "Party One                0\n",
            "Party Two                0\n",
            "dtype: int64\n",
            "\n",
            "Missing ground truth values in test_data before further processing:\n",
            "Aggrement Value          0\n",
            "Aggrement Start Date     0\n",
            "Aggrement End Date       0\n",
            "Renewal Notice (Days)    0\n",
            "Party One                0\n",
            "Party Two                0\n",
            "dtype: int64\n",
            "\n",
            "--- Data Preprocessing & Ground Truth Alignment Complete ---\n",
            "Stored 'train_data' (DataFrame)\n",
            "Stored 'test_data' (DataFrame)\n",
            "Stored 'TARGET_FIELDS' (list)\n",
            "Merged data and target fields stored for next phase.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Phase 3: ML-based Information Extraction (Setup + Pseudo-Annotation) ---\n",
        "\n",
        "print(\"\\n--- Starting ML-based Information Extraction (Phase 3) ---\")\n",
        "\n",
        "# Install required libraries\n",
        "print(\"\\nInstalling required libraries...\")\n",
        "!pip install transformers datasets accelerate evaluate seqeval -q\n",
        "!pip install fuzzywuzzy python-Levenshtein -q\n",
        "\n",
        "print(\"Libraries installed successfully.\")\n",
        "\n",
        "# Load previously stored data\n",
        "%store -r train_data\n",
        "%store -r test_data\n",
        "\n",
        "print(\"Train and test data loaded.\")\n",
        "\n",
        "# Fields we want to extract\n",
        "TARGET_FIELDS = [\n",
        "    'Aggrement Value',\n",
        "    'Aggrement Start Date',\n",
        "    'Aggrement End Date',\n",
        "    'Renewal Notice (Days)',\n",
        "    'Party One',\n",
        "    'Party Two'\n",
        "]\n",
        "print(f\"Target fields: {TARGET_FIELDS}\")\n",
        "\n",
        "# NER tag setup\n",
        "NER_TAG_PREFIXES = {\n",
        "    'Aggrement Value': 'AGREEMENT_VALUE',\n",
        "    'Aggrement Start Date': 'AGREEMENT_START_DATE',\n",
        "    'Aggrement End Date': 'AGREEMENT_END_DATE',\n",
        "    'Renewal Notice (Days)': 'RENEWAL_NOTICE_DAYS',\n",
        "    'Party One': 'PARTY_ONE',\n",
        "    'Party Two': 'PARTY_TWO'\n",
        "}\n",
        "\n",
        "all_ner_tags = ['O']\n",
        "for prefix in NER_TAG_PREFIXES.values():\n",
        "    all_ner_tags.append(f\"B-{prefix}\")\n",
        "    all_ner_tags.append(f\"I-{prefix}\")\n",
        "\n",
        "tag_to_id = {tag: i for i, tag in enumerate(all_ner_tags)}\n",
        "id_to_tag = {i: tag for tag, i in tag_to_id.items()}\n",
        "\n",
        "print(f\"NER tag list prepared: {all_ner_tags}\")\n",
        "\n",
        "# Imports for processing\n",
        "from datetime import datetime\n",
        "import re\n",
        "from fuzzywuzzy import fuzz, process\n",
        "import unicodedata\n",
        "\n",
        "# Helper to add ordinal suffix to a number\n",
        "def ordinal(n):\n",
        "    if 10 <= n % 100 <= 20:\n",
        "        return str(n) + 'th'\n",
        "    else:\n",
        "        return str(n) + {1: 'st', 2: 'nd', 3: 'rd'}.get(n % 10, 'th')\n",
        "\n",
        "# Clean and expand ground truth values for matching\n",
        "def normalize_gt_for_matching(field_name, value):\n",
        "    if pd.isna(value) or str(value).strip() == '':\n",
        "        return []\n",
        "\n",
        "    value_str = str(value).strip()\n",
        "    possible_matches = []\n",
        "\n",
        "    base_normalized = re.sub(r'\\s+', ' ', value_str).strip().lower()\n",
        "    possible_matches.append(base_normalized)\n",
        "\n",
        "    cleaned = re.sub(r'[^a-zA-Z0-9\\s]', '', unicodedata.normalize('NFKD', value_str).encode('ascii', 'ignore').decode('utf-8')).strip()\n",
        "    cleaned = re.sub(r'\\s+', ' ', cleaned).lower()\n",
        "    possible_matches.append(cleaned)\n",
        "    possible_matches.append(cleaned.replace(' ', ''))\n",
        "\n",
        "    if 'Date' in field_name:\n",
        "        try:\n",
        "            dt_formats = [\n",
        "                '%d.%m.%Y', '%d/%m/%Y', '%Y-%m-%d', '%m/%d/%Y',\n",
        "                '%d-%b-%Y', '%B %d, %Y', '%d %B %Y', '%b %d, %Y',\n",
        "                '%d %B, %Y', '%d %b, %Y', '%Y/%m/%d', '%Y.%m.%d'\n",
        "            ]\n",
        "            dt_obj = None\n",
        "            for fmt in dt_formats:\n",
        "                try:\n",
        "                    dt_obj = datetime.strptime(value_str, fmt)\n",
        "                    break\n",
        "                except ValueError:\n",
        "                    continue\n",
        "\n",
        "            if dt_obj:\n",
        "                output_formats = [\n",
        "                    '%d.%m.%Y', '%d/%m/%Y', '%Y-%m-%d', '%m/%d/%Y',\n",
        "                    '%B %d, %Y', '%d %B %Y', '%b %d, %Y', '%d %b %Y',\n",
        "                    '%Y/%m/%d', '%Y.%m.%d'\n",
        "                ]\n",
        "                for fmt in output_formats:\n",
        "                    try:\n",
        "                        formatted = dt_obj.strftime(fmt)\n",
        "                        possible_matches.append(re.sub(r'\\s+', ' ', formatted).strip().lower())\n",
        "                    except ValueError:\n",
        "                        pass\n",
        "\n",
        "                day_with_suffix = ordinal(dt_obj.day)\n",
        "                month_full = dt_obj.strftime('%B')\n",
        "                month_abbr = dt_obj.strftime('%b')\n",
        "                year = dt_obj.year\n",
        "\n",
        "                possible_matches.extend([\n",
        "                    f\"{day_with_suffix} day of {month_full} {year}\".lower(),\n",
        "                    f\"{day_with_suffix} day of {month_abbr} {year}\".lower(),\n",
        "                    f\"{day_with_suffix} of {month_full} {year}\".lower(),\n",
        "                    f\"{month_full} {dt_obj.day}, {year}\".lower(),\n",
        "                    f\"{month_abbr} {dt_obj.day}, {year}\".lower(),\n",
        "                    f\"{month_full} {day_with_suffix}, {year}\".lower()\n",
        "                ])\n",
        "\n",
        "                possible_matches = [re.sub(r'\\s+', ' ', s).strip() for s in possible_matches]\n",
        "\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "    elif 'Value' in field_name or 'Days' in field_name:\n",
        "        cleaned_value = value_str.lower().replace('usd', '').replace('rs', '').replace('inr', '').replace('p', '').strip()\n",
        "        cleaned_value = re.sub(r'[^0-9.,]', '', cleaned_value)\n",
        "\n",
        "        try:\n",
        "            num_for_float = re.sub(r'[^0-9.]', '', cleaned_value.replace(',', ''))\n",
        "            float_val = float(num_for_float)\n",
        "            possible_matches.append(str(float_val).lower())\n",
        "            if float_val == int(float_val):\n",
        "                possible_matches.append(str(int(float_val)).lower())\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "        if 'Value' in field_name:\n",
        "            possible_matches.extend([\n",
        "                'rs. ' + cleaned_value,\n",
        "                'rs ' + cleaned_value,\n",
        "                'p ' + cleaned_value,\n",
        "                'inr ' + cleaned_value,\n",
        "                '₹ ' + cleaned_value\n",
        "            ])\n",
        "        if 'Days' in field_name:\n",
        "            possible_matches.extend([\n",
        "                cleaned_value + ' days',\n",
        "                cleaned_value + ' day'\n",
        "            ])\n",
        "\n",
        "    elif 'Party' in field_name:\n",
        "        cleaned_name = re.sub(r'[^a-zA-Z0-9\\s]', '', unicodedata.normalize('NFKD', value_str).encode('ascii', 'ignore').decode('utf-8')).strip()\n",
        "        cleaned_name = re.sub(r'\\s+', ' ', cleaned_name).strip().lower()\n",
        "        possible_matches.append(cleaned_name)\n",
        "\n",
        "        if \"and/or\" in value_str.lower():\n",
        "            parties = value_str.lower().split(\"and/or\")\n",
        "            for p in parties:\n",
        "                cleaned_p = re.sub(r'[^a-zA-Z0-9\\s]', '', unicodedata.normalize('NFKD', p).encode('ascii', 'ignore').decode('utf-8')).strip()\n",
        "                cleaned_p = re.sub(r'\\s+', ' ', cleaned_p).strip()\n",
        "                if cleaned_p:\n",
        "                    possible_matches.append(cleaned_p)\n",
        "\n",
        "    return list(set([s for s in possible_matches if s]))\n",
        "\n",
        "\n",
        "def pseudo_annotate_document_refined(document_text, ground_truth_row, ner_tag_prefixes, tag_to_id, fuzzy_match_threshold=85):\n",
        "    annotations = []\n",
        "    text_original = document_text if document_text else \"\"\n",
        "    text_lower_cleaned = re.sub(r'[^a-zA-Z0-9\\s]', '', unicodedata.normalize('NFKD', text_original).encode('ascii', 'ignore').decode('utf-8')).strip()\n",
        "    text_lower_cleaned = re.sub(r'\\s+', ' ', text_lower_cleaned).lower()\n",
        "\n",
        "    temp_annotations = []\n",
        "\n",
        "    for field, tag_prefix in ner_tag_prefixes.items():\n",
        "        gt_value = ground_truth_row.get(field)\n",
        "        if pd.isna(gt_value) or str(gt_value).strip() == '':\n",
        "            continue\n",
        "\n",
        "        original_gt_str = str(gt_value).strip()\n",
        "        search_strings = normalize_gt_for_matching(field, original_gt_str)\n",
        "\n",
        "        best_match_idx = -1\n",
        "        best_match_len = -1\n",
        "        matched_text = \"\"\n",
        "\n",
        "        for search_str in sorted(search_strings, key=len, reverse=True):\n",
        "            if not search_str: continue\n",
        "\n",
        "            start_idx = text_original.lower().find(search_str)\n",
        "\n",
        "            if start_idx != -1:\n",
        "                actual_text = text_original[start_idx : start_idx + len(search_str)]\n",
        "                best_match_idx = start_idx\n",
        "                best_match_len = len(search_str)\n",
        "                matched_text = actual_text\n",
        "                break\n",
        "\n",
        "        if best_match_idx != -1:\n",
        "            end_idx = best_match_idx + best_match_len\n",
        "            temp_annotations.append({\n",
        "                'start': best_match_idx,\n",
        "                'end': end_idx,\n",
        "                'label': f\"B-{tag_prefix}\",\n",
        "                'value': original_gt_str,\n",
        "                'found_text': matched_text\n",
        "            })\n",
        "        else:\n",
        "            print(f\"  INFO: '{original_gt_str}' not found for '{field}' in file '{ground_truth_row['File Name']}'\")\n",
        "\n",
        "    annotations.extend(sorted(temp_annotations, key=lambda x: x['start']))\n",
        "    return annotations\n",
        "\n",
        "\n",
        "# Run pseudo-annotation on training data\n",
        "train_annotated_data = []\n",
        "for index, row in train_data.iterrows():\n",
        "    annotations = pseudo_annotate_document_refined(row['extracted_text'], row, NER_TAG_PREFIXES, tag_to_id)\n",
        "    train_annotated_data.append({\n",
        "        'id': str(index),\n",
        "        'document_text': row['extracted_text'],\n",
        "        'annotations': annotations,\n",
        "        'original_row': row.to_dict()\n",
        "    })\n",
        "\n",
        "print(f\"Annotated {len(train_annotated_data)} documents.\")\n",
        "\n",
        "# Show an example annotation\n",
        "if train_annotated_data:\n",
        "    first_doc = train_annotated_data[0]\n",
        "    snippet = first_doc['document_text'][:500].replace('\\n', ' ')\n",
        "    print(f\"\\nFile: {first_doc['original_row']['File Name']}\")\n",
        "    print(f\"Text Preview: {snippet}...\")\n",
        "    print(\"Annotations:\")\n",
        "    if first_doc['annotations']:\n",
        "        for ann in first_doc['annotations']:\n",
        "            context_start = max(0, ann['start'] - 20)\n",
        "            context_end = min(len(first_doc['document_text']), ann['end'] + 20)\n",
        "            context = first_doc['document_text'][context_start:context_end].replace('\\n', ' ')\n",
        "            print(f\"  Label: {ann['label']}, Value: '{ann['value']}' (Found: '{ann['found_text']}', Context: '...{context}...')\")\n",
        "    else:\n",
        "        print(\"  No annotations found.\")\n",
        "else:\n",
        "    print(\"No training data available.\")\n",
        "\n",
        "print(\"\\n--- Pseudo-Annotation Complete. Ready for tokenization ---\")\n",
        "\n",
        "# Save for next step\n",
        "%store train_annotated_data\n",
        "%store all_ner_tags\n",
        "%store tag_to_id\n",
        "%store id_to_tag\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUU__EG-zHUa",
        "outputId": "2aabc93f-3ce7-450d-a749-7981ff32372a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting ML-based Information Extraction (Phase 3) ---\n",
            "\n",
            "Installing required libraries...\n",
            "Libraries installed successfully.\n",
            "Train and test data loaded.\n",
            "Target fields: ['Aggrement Value', 'Aggrement Start Date', 'Aggrement End Date', 'Renewal Notice (Days)', 'Party One', 'Party Two']\n",
            "NER tag list prepared: ['O', 'B-AGREEMENT_VALUE', 'I-AGREEMENT_VALUE', 'B-AGREEMENT_START_DATE', 'I-AGREEMENT_START_DATE', 'B-AGREEMENT_END_DATE', 'I-AGREEMENT_END_DATE', 'B-RENEWAL_NOTICE_DAYS', 'I-RENEWAL_NOTICE_DAYS', 'B-PARTY_ONE', 'I-PARTY_ONE', 'B-PARTY_TWO', 'I-PARTY_TWO']\n",
            "  INFO: '6500' not found for 'Aggrement Value' in file '6683127-House-Rental-Contract-GERALDINE-GALINATO-v2-Page-1'\n",
            "  INFO: '20.05.2008' not found for 'Aggrement End Date' in file '6683127-House-Rental-Contract-GERALDINE-GALINATO-v2-Page-1'\n",
            "  INFO: '15.0' not found for 'Renewal Notice (Days)' in file '6683127-House-Rental-Contract-GERALDINE-GALINATO-v2-Page-1'\n",
            "  INFO: 'Antonio Levy S. Ingles, Jr. and/or Mary Rose C. Ingles' not found for 'Party One' in file '6683127-House-Rental-Contract-GERALDINE-GALINATO-v2-Page-1'\n",
            "  INFO: 'GERALDINE Q. GALINATO' not found for 'Party Two' in file '6683127-House-Rental-Contract-GERALDINE-GALINATO-v2-Page-1'\n",
            "  INFO: '6500' not found for 'Aggrement Value' in file '6683129-House-Rental-Contract-Geraldine-Galinato-v2'\n",
            "  INFO: '20.05.2008' not found for 'Aggrement End Date' in file '6683129-House-Rental-Contract-Geraldine-Galinato-v2'\n",
            "  INFO: 'Antonio Levy S. Ingles, Jr. and/or Mary Rose C. Ingles' not found for 'Party One' in file '6683129-House-Rental-Contract-Geraldine-Galinato-v2'\n",
            "  INFO: '4000' not found for 'Aggrement Value' in file '18325926-Rental-Agreement-1'\n",
            "  INFO: '05.12.2008' not found for 'Aggrement Start Date' in file '18325926-Rental-Agreement-1'\n",
            "  INFO: '31.11.2009' not found for 'Aggrement End Date' in file '18325926-Rental-Agreement-1'\n",
            "  INFO: '90.0' not found for 'Renewal Notice (Days)' in file '18325926-Rental-Agreement-1'\n",
            "  INFO: '01.05.2010' not found for 'Aggrement Start Date' in file '36199312-Rental-Agreement'\n",
            "  INFO: '31.04.2011' not found for 'Aggrement End Date' in file '36199312-Rental-Agreement'\n",
            "  INFO: '30.0' not found for 'Renewal Notice (Days)' in file '36199312-Rental-Agreement'\n",
            "  INFO: '3000' not found for 'Aggrement Value' in file '44737744-Maddireddy-Bhargava-Reddy-Rental-Agreement'\n",
            "  INFO: '20.09.2010' not found for 'Aggrement Start Date' in file '44737744-Maddireddy-Bhargava-Reddy-Rental-Agreement'\n",
            "  INFO: '19.07.2011' not found for 'Aggrement End Date' in file '44737744-Maddireddy-Bhargava-Reddy-Rental-Agreement'\n",
            "  INFO: 'M.V.V. VIJAYA SHANKAR' not found for 'Party One' in file '44737744-Maddireddy-Bhargava-Reddy-Rental-Agreement'\n",
            "  INFO: '31.02.2011' not found for 'Aggrement End Date' in file '47854715-RENTAL-AGREEMENT'\n",
            "  INFO: '10000' not found for 'Aggrement Value' in file '50070534-RENTAL-AGREEMENT'\n",
            "  INFO: '90.0' not found for 'Renewal Notice (Days)' in file '50070534-RENTAL-AGREEMENT'\n",
            "  INFO: '8000' not found for 'Aggrement Value' in file '54770958-Rental-Agreement'\n",
            "  INFO: '01.04.2011' not found for 'Aggrement Start Date' in file '54770958-Rental-Agreement'\n",
            "  INFO: '31.03.2012' not found for 'Aggrement End Date' in file '54770958-Rental-Agreement'\n",
            "  INFO: '90.0' not found for 'Renewal Notice (Days)' in file '54770958-Rental-Agreement'\n",
            "  INFO: 'K. Parthasarathy' not found for 'Party One' in file '54770958-Rental-Agreement'\n",
            "  INFO: 'Veerabrahmam Bathini' not found for 'Party Two' in file '54770958-Rental-Agreement'\n",
            "  INFO: '5500' not found for 'Aggrement Value' in file '54945838-Rental-Agreement'\n",
            "  INFO: '21.04.2011' not found for 'Aggrement Start Date' in file '54945838-Rental-Agreement'\n",
            "  INFO: '19.02.2012' not found for 'Aggrement End Date' in file '54945838-Rental-Agreement'\n",
            "  INFO: 'Asha Ramesh & Ramesh K.N' not found for 'Party One' in file '54945838-Rental-Agreement'\n",
            "  INFO: 'Sadasivuni Deepthi & Sadasivuni Kiran' not found for 'Party Two' in file '54945838-Rental-Agreement'\n",
            "Annotated 9 documents.\n",
            "\n",
            "File: 6683127-House-Rental-Contract-GERALDINE-GALINATO-v2-Page-1\n",
            "Text Preview: House Rental Contract KNOWN ALL MEN BY THESE PRESENTS: This House Rental Contract, made and entered into this 20th day of May 2007 at Manila by and between: Antonio Levy S. Ingles. Jr. and/or Mary Rose C. Ingles, of legal age, with residence and postal address at Unit 2006 EGI Taft Tower 2339 Taft Avenue, Malate, Manila, And herein referred to as the Owner(s), — And — GERALDINE O. GALINATO. of legal age, with residence and postal address at 6 Manganese Road, Pilar Village, Las Pinas, Metro Manil...\n",
            "Annotations:\n",
            "  Label: B-AGREEMENT_START_DATE, Value: '20.05.2007' (Found: '20th day of May 2007', Context: '...d entered into this 20th day of May 2007 at Manila by and be...')\n",
            "\n",
            "--- Pseudo-Annotation Complete. Ready for tokenization ---\n",
            "Stored 'train_annotated_data' (list)\n",
            "Stored 'all_ner_tags' (list)\n",
            "Stored 'tag_to_id' (dict)\n",
            "Stored 'id_to_tag' (dict)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Phase 3: ML-based Information Extraction (Tokenization & Model Fine-tuning) ---\n",
        "\n",
        "print(\"\\n--- Starting Tokenization and Model Fine-tuning ---\")\n",
        "\n",
        "# Load all necessary data from previous step\n",
        "%store -r train_annotated_data\n",
        "%store -r all_ner_tags\n",
        "%store -r tag_to_id\n",
        "%store -r id_to_tag\n",
        "%store -r test_data\n",
        "\n",
        "print(\"Loaded annotated training data and tag mappings.\")\n",
        "\n",
        "# Step 3.3: Prepare dataset for Hugging Face Transformers\n",
        "print(\"\\nPreparing dataset for Hugging Face Transformers...\")\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForTokenClassification\n",
        ")\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Choose your base model\n",
        "MODEL_CHECKPOINT = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "# Convert annotated documents to tokens and aligned NER tags\n",
        "def get_tokens_and_labels_for_hf(annotated_doc, tag_to_id_map):\n",
        "    text = annotated_doc['document_text']\n",
        "    annotations = annotated_doc['annotations']\n",
        "    if not text or not annotations:\n",
        "        return None\n",
        "\n",
        "    tokens, labels = [], []\n",
        "    annotations.sort(key=lambda x: x['start'])\n",
        "    current_char_idx = 0\n",
        "\n",
        "    for ann in annotations:\n",
        "        before_text = text[current_char_idx : ann['start']]\n",
        "        for word in before_text.split():\n",
        "            tokens.append(word)\n",
        "            labels.append(tag_to_id_map['O'])\n",
        "\n",
        "        entity_text = text[ann['start'] : ann['end']]\n",
        "        entity_words = entity_text.split()\n",
        "\n",
        "        if not entity_words:\n",
        "            current_char_idx = ann['end']\n",
        "            continue\n",
        "\n",
        "        tokens.append(entity_words[0])\n",
        "        labels.append(tag_to_id_map[ann['label']])\n",
        "        for word in entity_words[1:]:\n",
        "            labels.append(tag_to_id_map[ann['label'].replace('B-', 'I-')])\n",
        "            tokens.append(word)\n",
        "\n",
        "        current_char_idx = ann['end']\n",
        "\n",
        "    after_text = text[current_char_idx:]\n",
        "    for word in after_text.split():\n",
        "        tokens.append(word)\n",
        "        labels.append(tag_to_id_map['O'])\n",
        "\n",
        "    return {\"tokens\": tokens, \"ner_tags\": labels} if tokens else None\n",
        "\n",
        "# Process all annotated examples\n",
        "processed_train_examples = []\n",
        "for doc in train_annotated_data:\n",
        "    processed = get_tokens_and_labels_for_hf(doc, tag_to_id)\n",
        "    if processed:\n",
        "        processed_train_examples.append(processed)\n",
        "\n",
        "# Build Hugging Face dataset\n",
        "train_hf_dataset = Dataset.from_list(processed_train_examples)\n",
        "print(f\"Processed {len(processed_train_examples)} training examples.\")\n",
        "\n",
        "# Show sample\n",
        "if train_hf_dataset:\n",
        "    print(\"\\nSample tokenized example:\")\n",
        "    print(f\"Tokens: {train_hf_dataset[0]['tokens']}\")\n",
        "    print(f\"Labels: {[id_to_tag[i] for i in train_hf_dataset[0]['ner_tags']]}\")\n",
        "else:\n",
        "    print(\"No examples to show.\")\n",
        "\n",
        "# Align labels with tokens (handles subword splitting)\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        truncation=True,\n",
        "        is_split_into_words=True,\n",
        "    )\n",
        "\n",
        "    labels = []\n",
        "    for i, example_labels in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "\n",
        "        for word_id in word_ids:\n",
        "            if word_id is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_id != previous_word_idx:\n",
        "                label_ids.append(example_labels[word_id])\n",
        "            else:\n",
        "                current_tag = id_to_tag[example_labels[word_id]]\n",
        "                if current_tag.startswith(\"B-\"):\n",
        "                    label_ids.append(tag_to_id[current_tag.replace(\"B-\", \"I-\")])\n",
        "                else:\n",
        "                    label_ids.append(example_labels[word_id])\n",
        "            previous_word_idx = word_id\n",
        "\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Tokenize the dataset\n",
        "tokenized_train_dataset = train_hf_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "tokenized_train_dataset = tokenized_train_dataset.remove_columns(['tokens', 'ner_tags'])\n",
        "\n",
        "print(\"\\nDataset tokenized and aligned.\")\n",
        "print(f\"Input IDs: {tokenized_train_dataset[0]['input_ids']}\")\n",
        "print(f\"Labels: {[id_to_tag[i] if i != -100 else 'IGNORE' for i in tokenized_train_dataset[0]['labels']]}\")\n",
        "\n",
        "# Step 3.4: Load model and prepare for fine-tuning\n",
        "print(\"\\nLoading pre-trained model and setting up training...\")\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    MODEL_CHECKPOINT,\n",
        "    num_labels=len(all_ner_tags),\n",
        "    id2label=id_to_tag,\n",
        "    label2id=tag_to_id\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"no\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=False,\n",
        "    push_to_hub=False,\n",
        "    disable_tqdm=False,\n",
        "    fp16=torch.cuda.is_available(),\n",
        ")\n",
        "\n",
        "# Trainer handles training loop\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "print(\"\\nFine-tuning model (this might take some time)...\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n--- Fine-tuning Complete ---\")\n",
        "\n",
        "# Step 3.5: Save the model and tokenizer\n",
        "print(\"\\nSaving the fine-tuned model and tokenizer...\")\n",
        "output_model_dir = \"./fine_tuned_ner_model\"\n",
        "model.save_pretrained(output_model_dir)\n",
        "tokenizer.save_pretrained(output_model_dir)\n",
        "print(f\"Model saved to '{output_model_dir}'\")\n",
        "\n",
        "print(\"\\n--- Phase 3 (Tokenization & Model Fine-tuning) Complete ---\")\n",
        "\n",
        "# Store path for next use\n",
        "%store output_model_dir\n"
      ],
      "metadata": {
        "id": "xeJ1N0L0DC8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Diagnostic: Inspecting Pseudo-Annotated Training Data ---\n",
        "\n",
        "print(\"\\n--- Inspecting Pseudo-Annotated Training Data ---\")\n",
        "\n",
        "# Load necessary variables from previous steps\n",
        "%store -r train_annotated_data\n",
        "%store -r id_to_tag\n",
        "%store -r tag_to_id\n",
        "\n",
        "# Re-define tag prefixes in case not loaded\n",
        "NER_TAG_PREFIXES = {\n",
        "    'Aggrement Value': 'AGREEMENT_VALUE',\n",
        "    'Aggrement Start Date': 'AGREEMENT_START_DATE',\n",
        "    'Aggrement End Date': 'AGREEMENT_END_DATE',\n",
        "    'Renewal Notice (Days)': 'RENEWAL_NOTICE_DAYS',\n",
        "    'Party One': 'PARTY_ONE',\n",
        "    'Party Two': 'PARTY_TWO'\n",
        "}\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "if 'train_annotated_data' not in locals():\n",
        "    print(\"Error: Data not found. Please run Phase 3 Part 1 to generate annotations.\")\n",
        "else:\n",
        "    print(f\"Documents available: {len(train_annotated_data)}\")\n",
        "\n",
        "    total_annotations = sum(len(doc['annotations']) for doc in train_annotated_data)\n",
        "    print(f\"Total annotations generated: {total_annotations}\")\n",
        "\n",
        "    # Summarize annotation counts per document\n",
        "    print(\"\\n--- Annotations per Document (Summary) ---\")\n",
        "    summary = []\n",
        "    for doc in train_annotated_data:\n",
        "        file_name = doc['original_row']['File Name']\n",
        "        annotation_count = len(doc['annotations'])\n",
        "\n",
        "        field_count = {prefix: 0 for prefix in set(NER_TAG_PREFIXES.values())}\n",
        "        for ann in doc['annotations']:\n",
        "            if ann['label'].startswith('B-'):\n",
        "                prefix = ann['label'][2:]\n",
        "                if prefix in field_count:\n",
        "                    field_count[prefix] += 1\n",
        "\n",
        "        summary.append({\n",
        "            'File Name': file_name,\n",
        "            'Total Annotations': annotation_count,\n",
        "            'Field Counts': field_count\n",
        "        })\n",
        "\n",
        "    print(pd.DataFrame(summary).to_string())\n",
        "\n",
        "    # Show detailed annotations for first 3 documents with data\n",
        "    print(\"\\n--- Detailed Annotations (First 3 Non-Empty Documents) ---\")\n",
        "    shown = 0\n",
        "    for doc in train_annotated_data:\n",
        "        if shown >= 3:\n",
        "            break\n",
        "        if doc['annotations']:\n",
        "            print(f\"\\nDocument: {doc['original_row']['File Name']} (ID: {doc['id']})\")\n",
        "            snippet = doc['document_text'][:200].replace('\\n', ' ')\n",
        "            print(f\"Text Snippet: {snippet}...\")\n",
        "            print(\"Annotations:\")\n",
        "            for ann in doc['annotations']:\n",
        "                s = max(0, ann['start'] - 15)\n",
        "                e = min(len(doc['document_text']), ann['end'] + 15)\n",
        "                context = doc['document_text'][s:e].replace('\\n', ' ')\n",
        "                print(f\"  - Label: {ann['label']}, Value: '{ann['value']}' (Found: '{ann['found_text']}', Context: '...{context}...')\")\n",
        "            shown += 1\n",
        "\n",
        "    if shown == 0:\n",
        "        print(\"No documents with annotations available to show.\")\n",
        "\n",
        "print(\"\\n--- Inspection Complete ---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dt9aipfcXK_u",
        "outputId": "f7b76674-bb7c-4a9d-e8ed-106ef9d1e12b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Inspecting Pseudo-Annotated Training Data ---\n",
            "Documents available: 9\n",
            "Total annotations generated: 20\n",
            "\n",
            "--- Annotations per Document (Summary) ---\n",
            "                                                    File Name  Total Annotations                                                                                                                          Field Counts\n",
            "0  6683127-House-Rental-Contract-GERALDINE-GALINATO-v2-Page-1                  1  {'AGREEMENT_VALUE': 0, 'AGREEMENT_END_DATE': 0, 'PARTY_ONE': 0, 'RENEWAL_NOTICE_DAYS': 0, 'AGREEMENT_START_DATE': 1, 'PARTY_TWO': 0}\n",
            "1         6683129-House-Rental-Contract-Geraldine-Galinato-v2                  3  {'AGREEMENT_VALUE': 0, 'AGREEMENT_END_DATE': 0, 'PARTY_ONE': 0, 'RENEWAL_NOTICE_DAYS': 1, 'AGREEMENT_START_DATE': 1, 'PARTY_TWO': 1}\n",
            "2                                 18325926-Rental-Agreement-1                  2  {'AGREEMENT_VALUE': 0, 'AGREEMENT_END_DATE': 0, 'PARTY_ONE': 1, 'RENEWAL_NOTICE_DAYS': 0, 'AGREEMENT_START_DATE': 0, 'PARTY_TWO': 1}\n",
            "3                                   36199312-Rental-Agreement                  3  {'AGREEMENT_VALUE': 1, 'AGREEMENT_END_DATE': 0, 'PARTY_ONE': 1, 'RENEWAL_NOTICE_DAYS': 0, 'AGREEMENT_START_DATE': 0, 'PARTY_TWO': 1}\n",
            "4         44737744-Maddireddy-Bhargava-Reddy-Rental-Agreement                  1  {'AGREEMENT_VALUE': 0, 'AGREEMENT_END_DATE': 0, 'PARTY_ONE': 0, 'RENEWAL_NOTICE_DAYS': 0, 'AGREEMENT_START_DATE': 0, 'PARTY_TWO': 1}\n",
            "5                                   47854715-RENTAL-AGREEMENT                  5  {'AGREEMENT_VALUE': 1, 'AGREEMENT_END_DATE': 0, 'PARTY_ONE': 1, 'RENEWAL_NOTICE_DAYS': 1, 'AGREEMENT_START_DATE': 1, 'PARTY_TWO': 1}\n",
            "6                                   50070534-RENTAL-AGREEMENT                  4  {'AGREEMENT_VALUE': 0, 'AGREEMENT_END_DATE': 1, 'PARTY_ONE': 1, 'RENEWAL_NOTICE_DAYS': 0, 'AGREEMENT_START_DATE': 1, 'PARTY_TWO': 1}\n",
            "7                                   54770958-Rental-Agreement                  0  {'AGREEMENT_VALUE': 0, 'AGREEMENT_END_DATE': 0, 'PARTY_ONE': 0, 'RENEWAL_NOTICE_DAYS': 0, 'AGREEMENT_START_DATE': 0, 'PARTY_TWO': 0}\n",
            "8                                   54945838-Rental-Agreement                  1  {'AGREEMENT_VALUE': 0, 'AGREEMENT_END_DATE': 0, 'PARTY_ONE': 0, 'RENEWAL_NOTICE_DAYS': 1, 'AGREEMENT_START_DATE': 0, 'PARTY_TWO': 0}\n",
            "\n",
            "--- Detailed Annotations (First 3 Non-Empty Documents) ---\n",
            "\n",
            "Document: 6683127-House-Rental-Contract-GERALDINE-GALINATO-v2-Page-1 (ID: 0)\n",
            "Text Snippet: House Rental Contract KNOWN ALL MEN BY THESE PRESENTS: This House Rental Contract, made and entered into this 20th day of May 2007 at Manila by and between: Antonio Levy S. Ingles. Jr. and/or Mary Ros...\n",
            "Annotations:\n",
            "  - Label: B-AGREEMENT_START_DATE, Value: '20.05.2007' (Found: '20th day of May 2007', Context: '...ered into this 20th day of May 2007 at Manila by a...')\n",
            "\n",
            "Document: 6683129-House-Rental-Contract-Geraldine-Galinato-v2 (ID: 1)\n",
            "Text Snippet:      House Rental Contract KNOWN ALL MEN BY THESE PRESENTS: This House Rental Contract, made and entered into this 20th day of May 2007 at Manila by and between: Antonio Levy S. Ingles. Jr. and/or Mar...\n",
            "Annotations:\n",
            "  - Label: B-AGREEMENT_START_DATE, Value: '20.05.2007' (Found: '20th day of May 2007', Context: '...ered into this 20th day of May 2007 at Manila by a...')\n",
            "  - Label: B-RENEWAL_NOTICE_DAYS, Value: '15.0' (Found: '15', Context: '...unoccupied for 15 days while ren...')\n",
            "  - Label: B-PARTY_TWO, Value: 'GERALDINE Q. GALINATO' (Found: 'GERALDINE Q. GALINATO', Context: '...007, at Manila GERALDINE Q. GALINATO Resident CELES...')\n",
            "\n",
            "Document: 18325926-Rental-Agreement-1 (ID: 2)\n",
            "Text Snippet:           RENTAL AGREEMENT This deed of rental agreement executed at Bangalore this fifth day of December 2008 between MR.K.Kuttan S/o Kelu Aehari (Late) residing at site No 152 Geethalayam OMH colong...\n",
            "Annotations:\n",
            "  - Label: B-PARTY_ONE, Value: 'MR.K.Kuttan' (Found: 'MR.K.Kuttan', Context: '...r 2008 between MR.K.Kuttan S/o Kelu Aehar...')\n",
            "  - Label: B-PARTY_TWO, Value: 'P.M. Narayana Namboodri' (Found: 'P.M. Narayana Namboodri', Context: '...ssign and Sri. P.M. Narayana Namboodri “Laxmi Leela” ...')\n",
            "\n",
            "--- Inspection Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Phase 4: Metadata Extraction using Gemini API ---\n",
        "\n",
        "print(\"\\n--- Starting Metadata Extraction using Gemini API ---\")\n",
        "\n",
        "# 4.1 Install and Configure Google Generative AI SDK\n",
        "print(\"\\n4.1 Installing Google Generative AI SDK...\")\n",
        "!pip install -q google-generativeai\n",
        "\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Load required data from previous phases\n",
        "%store -r test_data\n",
        "%store -r TARGET_FIELDS\n",
        "print(\"Recalled test data and target fields.\")\n",
        "\n",
        "# Set your Gemini API key here (replace with your actual key)\n",
        "API_KEY = \"AIzaSyB6OAtSRJ0aGeKmLGVo4BTMhpPRsmCXFs4\"\n",
        "genai.configure(api_key=API_KEY)\n",
        "\n",
        "# Initialize the Gemini model\n",
        "GEMINI_MODEL_NAME = \"models/gemini-2.5-pro\"\n",
        "model_gemini = genai.GenerativeModel(GEMINI_MODEL_NAME)\n",
        "print(f\"Gemini model '{GEMINI_MODEL_NAME}' initialized.\")\n",
        "\n",
        "# 4.2 Define function to call Gemini API and parse output\n",
        "print(\"\\n4.2 Defining Gemini API call and parsing function...\")\n",
        "\n",
        "def extract_metadata_with_gemini(document_text, model, target_fields):\n",
        "    \"\"\"\n",
        "    Extracts metadata from document text using Gemini.\n",
        "    Returns a dictionary with all target fields (None if not found).\n",
        "    \"\"\"\n",
        "    if not document_text or len(document_text.strip()) < 50:\n",
        "        return {field: None for field in target_fields}\n",
        "\n",
        "    fields_str = \", \".join([f'\"{field}\"' for field in target_fields])\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert metadata extractor. Extract the following fields from the document below:\n",
        "    {fields_str}\n",
        "\n",
        "    Return the result as a JSON object. Use null if a field is not found.\n",
        "\n",
        "    Document:\n",
        "    ---\n",
        "    {document_text}\n",
        "    ---\n",
        "\n",
        "    JSON Output:\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        config = genai.GenerationConfig(temperature=0.0, top_p=1.0)\n",
        "        response = model.generate_content(prompt, generation_config=config)\n",
        "        response_text = response.text.strip()\n",
        "\n",
        "        # Extract JSON string from response (if wrapped in markdown)\n",
        "        match = re.search(r\"```json\\n(.*)\\n```\", response_text, re.DOTALL)\n",
        "        json_str = match.group(1) if match else response_text\n",
        "\n",
        "        extracted_data = json.loads(json_str)\n",
        "\n",
        "        # Normalize and match Gemini keys to our expected field names\n",
        "        final_output = {}\n",
        "        for field in target_fields:\n",
        "            key_plain = field.lower().replace(' ', '').replace('(', '').replace(')', '').replace('.', '')\n",
        "            key_underscore = field.lower().replace(' ', '_').replace('(', '').replace(')', '').replace('.', '')\n",
        "\n",
        "            val = extracted_data.get(field)\n",
        "            if val is None:\n",
        "                # Try case-insensitive fallback matching\n",
        "                for k in extracted_data:\n",
        "                    k_lower = k.lower()\n",
        "                    if k_lower in [field.lower(), key_plain, key_underscore]:\n",
        "                        val = extracted_data[k]\n",
        "                        break\n",
        "\n",
        "            final_output[field] = str(val).strip() if val and str(val).strip().lower() not in ['null', ''] else None\n",
        "\n",
        "        return final_output\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"JSON parse error (document starts with): '{document_text[:100]}...': {e}\")\n",
        "        return {field: None for field in target_fields}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Gemini error (document starts with): '{document_text[:100]}...': {e}\")\n",
        "        return {field: None for field in target_fields}\n",
        "\n",
        "# 4.3 Run metadata extraction for all test documents\n",
        "print(\"\\n4.3 Generating predictions for test documents using Gemini API...\")\n",
        "\n",
        "gemini_predicted_metadata_list = []\n",
        "for index, row in test_data.iterrows():\n",
        "    file_name = row['File Name']\n",
        "    extracted_text = row['extracted_text']\n",
        "\n",
        "    print(f\"  Processing '{file_name}'...\")\n",
        "    prediction = extract_metadata_with_gemini(extracted_text, model_gemini, TARGET_FIELDS)\n",
        "    prediction['File Name'] = file_name\n",
        "    gemini_predicted_metadata_list.append(prediction)\n",
        "\n",
        "# Convert predictions to DataFrame\n",
        "gemini_predictions_df = pd.DataFrame(gemini_predicted_metadata_list)\n",
        "print(\"\\nGemini Predictions Preview:\")\n",
        "print(gemini_predictions_df.head())\n",
        "\n",
        "print(\"\\n--- Gemini API Metadata Extraction Complete ---\")\n",
        "\n",
        "# Save predictions for evaluation phase\n",
        "%store gemini_predictions_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "id": "OxasUtpJ5rhk",
        "outputId": "a7897483-884d-4d9b-807d-f670c477f166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Metadata Extraction using Gemini API ---\n",
            "\n",
            "4.1 Installing Google Generative AI SDK...\n",
            "Recalled test data and target fields.\n",
            "Gemini model 'models/gemini-2.5-pro' initialized.\n",
            "\n",
            "4.2 Defining Gemini API call and parsing function...\n",
            "\n",
            "4.3 Generating predictions for test documents using Gemini API...\n",
            "  Processing '24158401-Rental-Agreement'...\n",
            "  Processing '95980236-Rental-Agreement'...\n",
            "  Processing '156155545-Rental-Agreement-Kns-Home'...\n",
            "  Processing '228094620-Rental-Agreement'...\n",
            "\n",
            "Gemini Predictions Preview:\n",
            "  Aggrement Value Aggrement Start Date Aggrement End Date  \\\n",
            "0           12000           2008-04-01         2009-03-31   \n",
            "1            9000           2010-04-01         2011-02-28   \n",
            "2           12000           2012-12-15         2013-11-14   \n",
            "3         15000.0         July 7, 2013       June 6, 2014   \n",
            "\n",
            "  Renewal Notice (Days)           Party One  \\\n",
            "0                    60       Sri Hanumaiah   \n",
            "1                  None   Mrs. S.Sakunthala   \n",
            "2                    30         V.K.NATARAJ   \n",
            "3                  None  Mr. KAPIL MEHROTRA   \n",
            "\n",
            "                                      Party Two  \\\n",
            "0                           Sri Vishal Bhardwaj   \n",
            "1                                 V.V Ravi Kian   \n",
            "2  SRI VYSHNAVI DAIRY SPECIALITIES Private Ltd.   \n",
            "3                                  Mr.B.Kishore   \n",
            "\n",
            "                             File Name  \n",
            "0            24158401-Rental-Agreement  \n",
            "1            95980236-Rental-Agreement  \n",
            "2  156155545-Rental-Agreement-Kns-Home  \n",
            "3           228094620-Rental-Agreement  \n",
            "\n",
            "--- Gemini API Metadata Extraction Complete ---\n",
            "Stored 'gemini_predictions_df' (DataFrame)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Phase 5: Evaluation & Final Conclusion ---\n",
        "\n",
        "print(\"\\n--- Starting Evaluation & Conclusion ---\")\n",
        "\n",
        "# 5.1 Load Required Data\n",
        "print(\"\\n5.1 Recalling test data, ground truth, and predictions...\")\n",
        "%store -r test_data\n",
        "%store -r gemini_predictions_df\n",
        "%store -r TARGET_FIELDS\n",
        "\n",
        "# Load ground truth from CSV\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "try:\n",
        "    test_df_gt = pd.read_csv(os.path.join(\"data\", \"test.csv\"))\n",
        "    print(\"Ground truth file 'test.csv' loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'test.csv' not found in the 'data/' directory.\")\n",
        "    exit()\n",
        "\n",
        "print(\"All data loaded for evaluation.\")\n",
        "\n",
        "# 5.2 Normalize Values for Matching\n",
        "print(\"\\n5.2 Normalizing values for comparison...\")\n",
        "\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "def normalize_value_for_comparison(field_name, value):\n",
        "    if value is None:\n",
        "        return None\n",
        "\n",
        "    value_str = str(value).strip()\n",
        "\n",
        "    # Handle dates\n",
        "    if \"Date\" in field_name:\n",
        "        date_formats = [\n",
        "            '%Y-%m-%d', '%d.%m.%Y', '%d/%m/%Y', '%m/%d/%Y',\n",
        "            '%B %d, %Y', '%d %B %Y', '%b %d, %Y', '%d %b %Y',\n",
        "            '%Y/%m/%d', '%Y.%m.%d'\n",
        "        ]\n",
        "        # Try direct parsing\n",
        "        for fmt in date_formats:\n",
        "            try:\n",
        "                return datetime.strptime(value_str, fmt).strftime('%Y-%m-%d')\n",
        "            except ValueError:\n",
        "                continue\n",
        "        # Try after removing ordinal suffixes\n",
        "        cleaned = re.sub(r'(\\d+)(st|nd|rd|th)', r'\\1', value_str)\n",
        "        for fmt in date_formats:\n",
        "            try:\n",
        "                return datetime.strptime(cleaned, fmt).strftime('%Y-%m-%d')\n",
        "            except ValueError:\n",
        "                continue\n",
        "        return re.sub(r'\\s+', ' ', value_str).lower()\n",
        "\n",
        "    # Handle numbers\n",
        "    elif \"Value\" in field_name or \"Days\" in field_name:\n",
        "        cleaned = re.sub(r'[^0-9.]', '', value_str.lower().replace(',', ''))\n",
        "        try:\n",
        "            return float(cleaned)\n",
        "        except ValueError:\n",
        "            return None\n",
        "\n",
        "    # Handle party names\n",
        "    elif \"Party\" in field_name:\n",
        "        cleaned = re.sub(r'[^a-zA-Z0-9\\s]', '', value_str)\n",
        "        return re.sub(r'\\s+', ' ', cleaned).strip().lower()\n",
        "\n",
        "    # Default normalization\n",
        "    return re.sub(r'\\s+', ' ', value_str).strip().lower()\n",
        "\n",
        "print(\"Normalization function ready.\")\n",
        "\n",
        "# 5.3 Evaluate Gemini Predictions\n",
        "print(\"\\n5.3 Evaluating Gemini predictions...\")\n",
        "\n",
        "# Prepare structure to store results\n",
        "recall_scores = {}\n",
        "evaluation_results = {\n",
        "    field: {\"true_matches\": 0, \"false_matches\": 0, \"not_extracted_in_gt\": 0}\n",
        "    for field in TARGET_FIELDS\n",
        "}\n",
        "\n",
        "# Merge predictions with ground truth\n",
        "evaluation_df = pd.merge(\n",
        "    test_df_gt, gemini_predictions_df,\n",
        "    on=\"File Name\", how=\"inner\", suffixes=(\"_gt\", \"_pred\")\n",
        ")\n",
        "\n",
        "print(f\"Documents evaluated: {len(evaluation_df)}\")\n",
        "\n",
        "# Perform per-field evaluation\n",
        "for _, row in evaluation_df.iterrows():\n",
        "    for field in TARGET_FIELDS:\n",
        "        gt_value = row.get(f\"{field}_gt\")\n",
        "        pred_value = row.get(f\"{field}_pred\")\n",
        "\n",
        "        norm_gt = normalize_value_for_comparison(field, gt_value)\n",
        "        norm_pred = normalize_value_for_comparison(field, pred_value)\n",
        "\n",
        "        if norm_gt is not None:\n",
        "            if norm_pred is not None and norm_gt == norm_pred:\n",
        "                evaluation_results[field][\"true_matches\"] += 1\n",
        "            else:\n",
        "                evaluation_results[field][\"false_matches\"] += 1\n",
        "        else:\n",
        "            evaluation_results[field][\"not_extracted_in_gt\"] += 1\n",
        "\n",
        "# Display per-field recall scores\n",
        "print(\"\\nPer-Field Recall Scores:\")\n",
        "for field, scores in evaluation_results.items():\n",
        "    true = scores[\"true_matches\"]\n",
        "    false = scores[\"false_matches\"]\n",
        "    total = true + false\n",
        "\n",
        "    if total > 0:\n",
        "        recall = true / total\n",
        "        print(f\"- {field}: {recall:.2f} (True: {true}, False: {false}, GT Total: {total})\")\n",
        "    else:\n",
        "        print(f\"- {field}: N/A (No ground truth available)\")\n",
        "\n",
        "print(\"\\n--- Evaluation Complete ---\")\n",
        "\n",
        "# 5.4 Generate Final Conclusion Text\n",
        "print(\"\\n--- Preparing Final Conclusion for README.md ---\")\n",
        "\n",
        "final_conclusion_text = f\"\"\"\n",
        "# AI/ML System for Metadata Extraction from Documents\n",
        "\n",
        "## 1. Problem Statement\n",
        "Develop an AI/ML system to extract the following metadata fields from documents:\n",
        "\n",
        "- Aggrement Value\n",
        "- Aggrement Start Date\n",
        "- Aggrement End Date\n",
        "- Renewal Notice (Days)\n",
        "- Party One\n",
        "- Party Two\n",
        "\n",
        "Manual rules were not allowed for the extraction system.\n",
        "\n",
        "## 2. Approach Overview\n",
        "\n",
        "### Phase 1: Text Extraction\n",
        "- `.docx`: Extracted via `python-docx`\n",
        "- Images (`.png`, `.jpg`): OCR via `pytesseract`\n",
        "- Robust handling for missing files\n",
        "\n",
        "### Phase 2: Dataset Alignment\n",
        "- Extracted text aligned with `train.csv` and `test.csv` to build structured datasets\n",
        "\n",
        "### Phase 3: BERT Fine-tuning (NER)\n",
        "- Used `bert-base-uncased` for token classification\n",
        "- Failed to learn effectively due to very limited data\n",
        "- Model defaulted to predicting all 'O' (outside) tags\n",
        "\n",
        "### Phase 4: Gemini API (LLM-based Extraction)\n",
        "- Sent full document text to Gemini API with prompt for JSON extraction\n",
        "- Output parsed and evaluated for accuracy\n",
        "- Bypassed need for large labeled training sets\n",
        "\n",
        "## 3. Evaluation Results\n",
        "\n",
        "Per-field recall (True / (True + False)):\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Append field-wise scores\n",
        "for field, results in evaluation_results.items():\n",
        "    true = results['true_matches']\n",
        "    false = results['false_matches']\n",
        "    total = true + false\n",
        "\n",
        "    if total > 0:\n",
        "        recall = true / total\n",
        "        final_conclusion_text += f\"- {field}: {recall:.2f} (True: {true}, False: {false})\\n\"\n",
        "    else:\n",
        "        final_conclusion_text += f\"- {field}: N/A (No GT available)\\n\"\n",
        "\n",
        "final_conclusion_text += \"\"\"\n",
        "## 4. Instructions to Run\n",
        "\n",
        "1. Clone repo and open the notebook in Google Colab\n",
        "2. Upload `train.csv` and `test.csv` into the `data/` folder\n",
        "3. Add document files to `data/train/` and `data/test/`\n",
        "4. Replace the placeholder with your Gemini API key\n",
        "5. Run cells sequentially\n",
        "\n",
        "## 5. Future Enhancements\n",
        "\n",
        "- Human-annotated training set for better fine-tuning\n",
        "- Advanced weak labeling for pseudo-annotations\n",
        "- Combine Gemini API with classical NLP models\n",
        "- Confidence scores per field\n",
        "- Optional FastAPI wrapper for RESTful use\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(final_conclusion_text)\n",
        "print(\"\\n--- Phase 5 Complete ---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WM1FwtRwMuDb",
        "outputId": "aecea978-b71d-4990-f98b-aaa04de832b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Evaluation & Conclusion ---\n",
            "\n",
            "5.1 Recalling test data, ground truth, and predictions...\n",
            "Ground truth file 'test.csv' loaded successfully.\n",
            "All data loaded for evaluation.\n",
            "\n",
            "5.2 Normalizing values for comparison...\n",
            "Normalization function ready.\n",
            "\n",
            "5.3 Evaluating Gemini predictions...\n",
            "Documents evaluated: 4\n",
            "\n",
            "Per-Field Recall Scores:\n",
            "- Aggrement Value: 1.00 (True: 4, False: 0, GT Total: 4)\n",
            "- Aggrement Start Date: 1.00 (True: 4, False: 0, GT Total: 4)\n",
            "- Aggrement End Date: 0.75 (True: 3, False: 1, GT Total: 4)\n",
            "- Renewal Notice (Days): 0.50 (True: 2, False: 2, GT Total: 4)\n",
            "- Party One: 0.25 (True: 1, False: 3, GT Total: 4)\n",
            "- Party Two: 0.00 (True: 0, False: 4, GT Total: 4)\n",
            "\n",
            "--- Evaluation Complete ---\n",
            "\n",
            "--- Preparing Final Conclusion for README.md ---\n",
            "\n",
            "# AI/ML System for Metadata Extraction from Documents\n",
            "\n",
            "## 1. Problem Statement\n",
            "Develop an AI/ML system to extract the following metadata fields from documents:\n",
            "\n",
            "- Aggrement Value\n",
            "- Aggrement Start Date\n",
            "- Aggrement End Date\n",
            "- Renewal Notice (Days)\n",
            "- Party One\n",
            "- Party Two\n",
            "\n",
            "Manual rules were not allowed for the extraction system.\n",
            "\n",
            "## 2. Approach Overview\n",
            "\n",
            "### Phase 1: Text Extraction\n",
            "- `.docx`: Extracted via `python-docx`\n",
            "- Images (`.png`, `.jpg`): OCR via `pytesseract`\n",
            "- Robust handling for missing files\n",
            "\n",
            "### Phase 2: Dataset Alignment\n",
            "- Extracted text aligned with `train.csv` and `test.csv` to build structured datasets\n",
            "\n",
            "### Phase 3: BERT Fine-tuning (NER)\n",
            "- Used `bert-base-uncased` for token classification\n",
            "- Failed to learn effectively due to very limited data\n",
            "- Model defaulted to predicting all 'O' (outside) tags\n",
            "\n",
            "### Phase 4: Gemini API (LLM-based Extraction)\n",
            "- Sent full document text to Gemini API with prompt for JSON extraction\n",
            "- Output parsed and evaluated for accuracy\n",
            "- Bypassed need for large labeled training sets\n",
            "\n",
            "## 3. Evaluation Results\n",
            "\n",
            "Per-field recall (True / (True + False)):\n",
            "\n",
            "- Aggrement Value: 1.00 (True: 4, False: 0)\n",
            "- Aggrement Start Date: 1.00 (True: 4, False: 0)\n",
            "- Aggrement End Date: 0.75 (True: 3, False: 1)\n",
            "- Renewal Notice (Days): 0.50 (True: 2, False: 2)\n",
            "- Party One: 0.25 (True: 1, False: 3)\n",
            "- Party Two: 0.00 (True: 0, False: 4)\n",
            "\n",
            "## 4. Instructions to Run\n",
            "\n",
            "1. Clone repo and open the notebook in Google Colab\n",
            "2. Upload `train.csv` and `test.csv` into the `data/` folder\n",
            "3. Add document files to `data/train/` and `data/test/`\n",
            "4. Replace the placeholder with your Gemini API key\n",
            "5. Run cells sequentially\n",
            "\n",
            "## 5. Future Enhancements\n",
            "\n",
            "- Human-annotated training set for better fine-tuning\n",
            "- Advanced weak labeling for pseudo-annotations\n",
            "- Combine Gemini API with classical NLP models\n",
            "- Confidence scores per field\n",
            "- Optional FastAPI wrapper for RESTful use\n",
            "\n",
            "\n",
            "\n",
            "--- Phase 5 Complete ---\n"
          ]
        }
      ]
    }
  ]
}